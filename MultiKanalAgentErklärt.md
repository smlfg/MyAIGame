# MultiKanal â€” Das komplette Projekt erklÃ¤rt

> **Dein KI-Coding-Team redet jetzt mit dir.** MultiKanal verwandelt stumme Terminal-Ausgaben in gesprochene deutsche Audio-Kommentare â€” fÃ¼r jeden Agenten eine eigene Stimme.

---

## 1. Die groÃŸe Idee

Stell dir vor, du hast vier Assistenten in einem BÃ¼ro â€” Claude Code, OpenCode, Codex, Gemini. Alle arbeiten gleichzeitig, aber keiner sagt was. Du musst stÃ¤ndig auf den Bildschirm starren, um mitzubekommen was passiert.

**MultiKanal lÃ¶st das:** Jeder Agent bekommt ein Mikrofon. Wenn Claude Code einen Bug fixt, *hÃ¶rst du*: "Auth-Bug gefixt, drei Dateien angepasst. Login sollte jetzt stabil laufen." Und du erkennst an der Stimme, *welcher* Agent spricht.

---

## 2. Architektur â€” Wie das System aufgebaut ist

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KI-Coding-Agenten                    â”‚
â”‚  Claude Code  â”‚  OpenCode  â”‚  Codex  â”‚  Gemini  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Hook JSON   â”‚ SSE      â”‚ JSONL   â”‚
       â–¼             â–¼          â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MultiKanal Daemon (Port 7742)               â”‚
â”‚      FastAPI + Uvicorn (async)                   â”‚
â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. FILTER â€” Noise raus                   â”‚    â”‚
â”‚  â”‚    Code-BlÃ¶cke, Pfade, ANSI, URLs â†’ weg  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. CACHE CHECK â€” Schon mal gehÃ¶rt?       â”‚    â”‚
â”‚  â”‚    SHA256(text+voice) â†’ .wav oder miss    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 3. NARRATION â€” LLM fasst zusammen        â”‚    â”‚
â”‚  â”‚    MiniMax â†’ Ollama â†’ Template â†’ Pass     â”‚    â”‚
â”‚  â”‚    System-Prompt: audio_prompt.md         â”‚    â”‚
â”‚  â”‚    (Watchdog â†’ hot-reload ohne Restart)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. TTS â€” Text wird Audio                 â”‚    â”‚
â”‚  â”‚    Edge TTS â†’ Piper â†’ spd-say            â”‚    â”‚
â”‚  â”‚    Pro Agent eigene Stimme + Speed/Pitch  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 5. AUDIO QUEUE â€” Einer nach dem anderen  â”‚    â”‚
â”‚  â”‚    asyncio.Queue(maxsize=5), FIFO         â”‚    â”‚
â”‚  â”‚    paplay â†’ ffplay â†’ aplay                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 6. EVAL LOG â€” QualitÃ¤t messen            â”‚    â”‚
â”‚  â”‚    info_density, filler_count, prompt_hashâ”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼  ğŸ”Š Lautsprecher
```

### Die 4 Schichten als Analogie

| Schicht | Rolle | Analogie |
|---------|-------|----------|
| **Adapter** | FÃ¤ngt Agent-Output ab | Mikrofon an jedem Agenten |
| **Narration** | KÃ¼rzt auf 1-2 SÃ¤tze | Redakteur, der den Text strafft |
| **TTS** | Wandelt Text â†’ Audio | Sprecher im Studio |
| **Playback** | Spielt ab, nie gleichzeitig | Regie, die Mikros schaltet |

---

## 3. Verzeichnisstruktur

```
AiSystemForVibeCoding/
â”œâ”€â”€ bin/                              # CLI-Tools (Bash)
â”‚   â”œâ”€â”€ ai                            # Kommando-ErklÃ¤rer ("ai grep" â†’ Power-User-Tipps)
â”‚   â””â”€â”€ ai-speak                      # Manuelles TTS-Frontend
â”‚
â”œâ”€â”€ config/                           # Konfiguration (hot-reloadable)
â”‚   â”œâ”€â”€ default.yaml                  # Hauptconfig: Ports, Voices, Provider-Chain
â”‚   â””â”€â”€ audio_prompt.md               # QualitÃ¤tsregeln fÃ¼r Narrations
â”‚
â”œâ”€â”€ src/multikanal/                   # Python-Paket (pip install -e .)
â”‚   â”œâ”€â”€ __main__.py                   # Entry: ruft cli.main() auf
â”‚   â”œâ”€â”€ cli.py                        # Subcommands: daemon, narrate, health, stop...
â”‚   â”œâ”€â”€ config.py                     # YAML-Loader mit .env-Injection
â”‚   â”œâ”€â”€ daemon.py                     # â˜… HERZSTÃœCK: FastAPI-Server
â”‚   â”‚
â”‚   â”œâ”€â”€ adapters/                     # Ein Adapter pro Agent
â”‚   â”‚   â”œâ”€â”€ base.py                   # Abstract: send_to_daemon(), _guess_language()
â”‚   â”‚   â”œâ”€â”€ claude_hook.py            # Claude Code Hook-Events verarbeiten
â”‚   â”‚   â”œâ”€â”€ codex_wrapper.py          # Codex JSONL-Stream wrappen
â”‚   â”‚   â””â”€â”€ opencode_sse.py           # OpenCode SSE-Events empfangen
â”‚   â”‚
â”‚   â”œâ”€â”€ narration/                    # Text â†’ Deutsche Zusammenfassung
â”‚   â”‚   â”œâ”€â”€ generator.py              # Provider-Chain orchestrieren
â”‚   â”‚   â”œâ”€â”€ providers.py              # MiniMax, Ollama, Template, Passthrough
â”‚   â”‚   â”œâ”€â”€ claude_code.py            # Claude CLI als Narrations-Provider
â”‚   â”‚   â”œâ”€â”€ filter.py                 # Noise rausfiltern (Code, Pfade, URLs)
â”‚   â”‚   â”œâ”€â”€ eval_log.py               # QualitÃ¤tsmetriken â†’ JSONL
â”‚   â”‚   â”œâ”€â”€ prompt.py                 # Watchdog: hot-reload audio_prompt.md
â”‚   â”‚   â””â”€â”€ template.py              # 50+ statische Fallback-Templates
â”‚   â”‚
â”‚   â””â”€â”€ tts/                          # Zusammenfassung â†’ Audio
â”‚       â”œâ”€â”€ piper.py                  # Edge TTS / Piper / spd-say Engine
â”‚       â”œâ”€â”€ cache.py                  # SHA256-basierter LRU-Cache
â”‚       â””â”€â”€ playback.py              # paplay / ffplay / aplay Abspieler
â”‚
â”œâ”€â”€ plugins/claude-hook/hooks/        # Claude Code Integration
â”‚   â”œâ”€â”€ stop.py                       # Feuert bei Session-Ende
â”‚   â””â”€â”€ post_tool_use.py              # Feuert nach jedem Tool-Aufruf
â”‚
â”œâ”€â”€ systemd/multikanal.service        # Autostart als systemd User-Service
â”œâ”€â”€ pyproject.toml                    # Dependencies & `multikanal` CLI Entry Point
â”œâ”€â”€ .env.example                      # Template: MINIMAX_API_KEY=...
â””â”€â”€ FOR_SMLFLG.md                     # Architektur-Doku
```

---

## 4. Deep Dive: Narration Pipeline

### Was passiert, wenn Text reinkommt

Die Narration Pipeline ist das Gehirn des Systems. Sie nimmt rohen Agent-Output und verwandelt ihn in einen knackigen deutschen Satz.

### 4.1 Filter (`narration/filter.py`)

**Problem:** Agent-Output ist voller Noise â€” ANSI-Codes, Code-BlÃ¶cke, Dateipfade, URLs, Diff-Header. Wenn du das einem LLM gibst, produziert es MÃ¼ll.

**LÃ¶sung:** 7 Regex-Passes in fester Reihenfolge:

```python
# Reihenfolge ist wichtig!
1. ANSI-Escape-Codes      â†’ weg (Farbcodes, Cursor-Steuerung)
2. Code-Fences ```...```   â†’ "[code block removed]"
3. Inline-Code `...`       â†’ weg
4. Tool-Call-JSON          â†’ weg ({"tool": ...})
5. Diff-Header (---/+++/@@)â†’ weg
6. Diff-Zeilen (+/-)       â†’ weg
7. Dateipfade (/foo/bar)   â†’ weg
8. URLs (http://...)       â†’ weg
9. Mehrfach-Leerzeilen     â†’ eine Leerzeile
10. Truncate auf 2000 Zeichen (am Wortende)
```

**Warum?** Der LLM soll nur den *Inhalt* sehen, nicht den *Code*. "3 files changed, 47 insertions" ist nÃ¼tzlich. "```python\ndef foo():\n  pass\n```" ist Noise.

### 4.2 Generator (`narration/generator.py`)

**Das Chain-of-Responsibility Pattern:** Der Generator probiert Provider der Reihe nach durch, bis einer eine Antwort liefert.

```python
class NarrationGenerator:
    def generate(self, text, system_prompt, language):
        for provider in self.providers:
            try:
                narration = provider.generate(text, system_prompt, language)
            except Exception:
                pass  # NÃ¤chster Provider
            if narration:
                self.last_result = {"provider": provider.name, "latency_ms": ...}
                return narration
        return ""  # Alle gescheitert
```

**Konfigurierbar via YAML:** Welche Provider aktiv sind, in welcher Reihenfolge, mit welchen Timeouts â€” alles in `config/default.yaml`.

### 4.3 Provider im Detail (`narration/providers.py`)

#### MiniMax (PrimÃ¤r â€” schnell, online)
```
Endpoint: https://api.minimax.io/v1/chat/completions
Modell:   MiniMax-M2.1
Timeout:  6 Sekunden
Kosten:   ~$0.15/1M tokens
```

**Wie es funktioniert:**
1. System-Prompt (aus `audio_prompt.md`) + User-Text â†’ POST an API
2. Antwort parsen (multiple Response-Shapes werden unterstÃ¼tzt â€” MiniMax Ã¤ndert sein Format gelegentlich)
3. `<think>`-Tags und Markdown-Formatierung rausstrippen â†’ sauberer TTS-Text

**Robustheit:** Der Response-Parser probiert 6 verschiedene JSON-Pfade:
```python
choice.message.content â†’ choice.delta.content â†’ choice.messages[0].content
â†’ choice.output_text â†’ data.text â†’ data.content
```
MiniMax hat sein API-Format schon mehrfach geÃ¤ndert. Dieser Parser Ã¼berlebt das.

#### Ollama (Fallback â€” langsam, lokal)
```
Endpoint: http://localhost:11434/api/generate
Modelle:  llama3.1:8b â†’ phi4 â†’ mistral-nemo (der Reihe nach)
Timeout:  10 Sekunden
Kosten:   Gratis (nur Strom)
```

**Prompt auf Deutsch:**
```
Agent-Ausgabe:
{text}

Erstelle eine Audio-ErklÃ¤rung (maximal 80 WÃ¶rter).
```

Probiert 3 Modelle der Reihe nach. Wenn das erste Modell nicht installiert ist â†’ nÃ¤chstes.

#### Template (Not-Fallback â€” instant, statisch)
Kein LLM nÃ¶tig. Nimmt den gefilterten Text, kÃ¼rzt auf max_words, stellt "Kurze Zusammenfassung:" voran. QualitÃ¤t mÃ¤ÃŸig, aber immer verfÃ¼gbar.

#### Passthrough (Letzter Ausweg)
Gibt den Input-Text direkt zurÃ¼ck, gekÃ¼rzt auf max_words. Kein LLM, kein Template. Einfach durchreichen.

### 4.4 Prompt Hot-Reload (`narration/prompt.py`)

**Das Problem:** Du willst den Narrations-Stil Ã¤ndern ("weniger FÃ¼llwÃ¶rter", "max 60 statt 40 WÃ¶rter"), ohne den Daemon neu zu starten.

**Die LÃ¶sung:** `PromptWatcher` nutzt die `watchdog`-Library:

```python
class PromptWatcher:
    def __init__(self, prompt_path):
        self._path = Path(prompt_path)
        self._prompt = ""
        self._lock = threading.Lock()  # Thread-safe!
        self._load()  # Sofort laden

    def start(self):
        # Watchdog Observer Ã¼berwacht das config-Verzeichnis
        observer = Observer()
        observer.schedule(Handler(), str(self._path.parent))
        observer.daemon = True  # Stirbt mit dem Daemon
        observer.start()
```

**Ablauf:**
1. Daemon startet â†’ `prompt.py` liest `config/audio_prompt.md`
2. Watchdog Ã¼berwacht das Verzeichnis
3. Du editierst die Datei (z.B. "max 40 WÃ¶rter" â†’ "max 60 WÃ¶rter")
4. OS feuert inotify-Event â†’ Watchdog Handler â†’ `_load()` wird aufgerufen
5. NÃ¤chste Narration nutzt den neuen Prompt
6. Thread-Lock verhindert Race Conditions beim Lesen

**Fallback:** Wenn `watchdog` nicht installiert ist, funktioniert alles trotzdem â€” nur ohne Auto-Reload. Du mÃ¼sstest den Daemon neu starten.

### 4.5 Eval-Logging (`narration/eval_log.py`)

**Warum?** Du willst *messen*, ob deine Prompt-Ã„nderungen die Narrations besser machen. BauchgefÃ¼hl reicht nicht.

**Was geloggt wird (JSONL, eine Zeile pro Narration):**

```json
{
  "ts": "2026-02-18T14:32:07Z",
  "source": "claude_code",
  "provider": "minimax",
  "in_chars": 847,
  "out_chars": 142,
  "out_words": 23,
  "prompt_hash": "a3f2c1e8",
  "llm_ms": 2847,
  "filler_count": 0,
  "starts_filler": false,
  "info_density": 0.78,
  "compression": 5.96,
  "input_preview": "Modified 3 files...",
  "narration": "Drei Dateien angepasst, Auth-Modul lÃ¤uft jetzt stabil."
}
```

**Die Metriken erklÃ¤rt:**

| Metrik | Was sie misst | Gut | Schlecht |
|--------|--------------|-----|---------|
| `info_density` | Anteil InhaltswÃ¶rter vs. StoppwÃ¶rter | >0.7 | <0.5 |
| `filler_count` | Verbotene Phrasen ("Es wurde", "GrundsÃ¤tzlich") | 0 | >1 |
| `starts_filler` | Beginnt mit FÃ¼llphrase? | false | true |
| `compression` | Input-Chars / Output-Chars | >3.0 | <1.5 |

**StoppwÃ¶rter-Liste:** 100+ deutsche FunktionswÃ¶rter (der, die, das, und, oder, aber...) die nicht als "Inhalt" zÃ¤hlen.

**`prompt_hash`:** SHA256 der ersten 8 Hex-Zeichen des System-Prompts. Damit kannst du filtern: "Zeig mir alle Narrations von Prompt-Version a3f2c1e8 vs. 7b4e9f12" â†’ A/B-Testing.

---

## 5. Deep Dive: Hook-System

### Die eiserne Regel

> **Hooks blockieren NIE den Agenten.** Egal was passiert â€” `sys.exit(0)`.

Jeder Hook ist in einen doppelten try/except gewickelt. Wenn der Daemon nicht lÃ¤uft? Exit 0. Wenn JSON-Parsing fehlschlÃ¤gt? Exit 0. Wenn HTTP timeout? Exit 0. Der Agent merkt nichts.

### 5.1 PostToolUse Hook (`plugins/claude-hook/hooks/post_tool_use.py`)

**Trigger:** Feuert *nach jedem Tool-Aufruf* in Claude Code â€” Bash, Edit, Write, etc.

**Der Ablauf:**

```
Claude Code fÃ¼hrt ein Tool aus (z.B. Bash: "git commit")
    â†“
Claude Code schreibt Event-JSON auf stdin des Hooks:
{
  "tool_name": "Bash",
  "tool_input": {"command": "git commit -m 'fix auth'", "description": "Commit changes"},
  "tool_response": {"stdout": "3 files changed, 47 insertions(+)"}
}
    â†“
Hook liest JSON, prÃ¼ft:
  - tool_name in SKIP_TOOLS? (Read, Glob, Grep, WebSearch, WebFetch â†’ ignorieren)
  - Nein? â†’ Weiter
    â†“
Extrahiert Text aus tool_response:
  - Probiert: content â†’ text â†’ output â†’ stdout â†’ result
  - FÃ¼r Bash: stdout ist der Hauptoutput
    â†“
Baut Kontext dazu:
  - Hat tool_input.description? â†’ Nutze das als Kontext
  - Sonst: "Command: git commit -m 'fix auth'"
    â†“
Kombiniert: "Commit changes\n\nResult:\n3 files changed, 47 insertions(+)"
    â†“
HTTP POST an localhost:7742/narrate (Timeout: 2 Sekunden)
  {"text": "...", "source": "claude_code"}
    â†“
Sofort exit 0 (wartet nicht auf Antwort!)
```

**Warum diese Tools Ã¼bersprungen werden:**
- `Read` â€” Gibt Dateiinhalt zurÃ¼ck â†’ riesig, irrelevant zum Vorlesen
- `Glob` â€” Gibt Dateilisten zurÃ¼ck â†’ Noise
- `Grep` â€” Gibt Suchergebnisse zurÃ¼ck â†’ zu technisch
- `WebSearch/WebFetch` â€” Gibt Webinhalte zurÃ¼ck â†’ zu lang

**Technisches Detail:** Der Hook nutzt `http.client` statt `httpx` oder `requests` â€” **null externe Dependencies**. Das ist Absicht: Hooks mÃ¼ssen instant starten, ohne `import httpx` (was ~100ms dauert).

### 5.2 Stop Hook (`plugins/claude-hook/hooks/stop.py`)

**Trigger:** Feuert wenn Claude Code eine Session beendet (letzte Antwort fertig).

**Der Ablauf:**

```
Claude Code beendet Session
    â†“
Stop-Event auf stdin:
{
  "stop_hook_active": false,
  "transcript_path": "/tmp/claude-transcript-abc123.jsonl"
}
    â†“
PrÃ¼ft: stop_hook_active? â†’ true = Exit (Loop-Prevention!)
    â†“
Liest Transcript JSONL rÃ¼ckwÃ¤rts (letzte 50 Zeilen):
  - Sucht nach role: "assistant"
  - Extrahiert Text-Content (kann String oder Array von BlÃ¶cken sein)
    â†“
Truncate auf 3000 Zeichen
    â†“
HTTP POST: {"text": "...", "source": "claude_stop"}
    â†“
Exit 0
```

**Loop-Prevention:** Ohne `stop_hook_active`-Check wÃ¼rde passieren:
1. Claude Code antwortet â†’ Stop Hook feuert
2. Daemon narrates â†’ Claude Code sieht das als neue AktivitÃ¤t?
3. Neuer Stop Hook â†’ endlos

Der `stop_hook_active`-Flag verhindert das.

**Transcript-Parsing:** Der JSONL-Parser ist defensiv programmiert â€” er probiert zwei Formate:
```python
# Format 1: Direkt
{"role": "assistant", "content": "Text..."}

# Format 2: Gewrappt
{"message": {"role": "assistant", "content": [{"type": "text", "text": "..."}]}}
```

---

## 6. Deep Dive: TTS & Audio

### 6.1 Stimmen-Zuordnung â€” Jeder Agent hat eine PersÃ¶nlichkeit

```
Claude Code  â†’ Florian (de-DE-FlorianMultilingualNeural)
                Ruhig, neutral, der "Standard-Kollege"

OpenCode     â†’ Conrad (de-DE-ConradNeural)
                Energisch, +5% Speed, fÃ¼r Live-Updates

Codex        â†’ Seraphina (de-DE-SeraphinaMultilingualNeural)
                Kreativ, +2Hz Pitch

Gemini       â†’ Killian (de-DE-KillianNeural)
                Analytisch

Englisch     â†’ Aria (en-US-AriaNeural)
```

**Warum verschiedene Stimmen?**
Stell dir vor, 3 Leute reden in einem Raum. Wenn alle die gleiche Stimme hÃ¤tten, wÃ¼sstest du nicht, wer spricht. Mit verschiedenen Stimmen weiÃŸt du *unterbewusst*, wer gerade dran ist â€” ohne auf den Bildschirm zu schauen.

**Per-Voice Tuning:**
```yaml
voice_settings:
  claude_code:  { rate: "+0%",  pitch: "+0Hz" }   # Baseline
  opencode:     { rate: "+5%",  pitch: "+0Hz" }   # Etwas schneller
  opencode_live:{ rate: "+8%",  pitch: "+0Hz" }   # Live-Updates noch schneller
  codex:        { rate: "+0%",  pitch: "+2Hz" }   # Etwas hÃ¶her
```

### 6.2 TTS Engine Chain (`tts/piper.py`)

**3-Stufen-Fallback, wie bei der Narration:**

#### Stufe 1: Edge TTS (Microsoft, online, beste QualitÃ¤t)
```python
async def _save():
    communicate = edge_tts.Communicate(text, voice, rate=rate, pitch=pitch)
    await communicate.save(outpath)

asyncio.run(_save())  # Sync-Wrapper um async Edge TTS
```

- Nutzt Microsoft Azure Neural Voices (kostenlos!)
- Rate und Pitch pro Voice konfigurierbar
- Produziert .wav Dateien
- Braucht Internet

#### Stufe 2: Piper (lokal, offline, Open Source)
```python
cmd = ["piper", "--model", "de-de-thorsten-medium.onnx", "--output_file", "out.wav"]
subprocess.run(cmd, input=text.encode(), timeout=30)
```

- Nur wenn .onnx Modelldatei vorhanden
- Thorsten-Modell fÃ¼r Deutsch
- `length_scale` steuert Geschwindigkeit (1/speed)
- Braucht kein Internet

#### Stufe 3: spd-say (Systemfallback, niedrigste QualitÃ¤t)
```python
cmd = ["/usr/bin/spd-say", "-w", outpath, "-r", rate, "-p", "50", "-l", "de", text]
```

- Speech Dispatcher (Linux-Standard)
- Klingt robotisch, aber funktioniert immer
- Letzter Ausweg

### 6.3 Audio-Cache (`tts/cache.py`)

**Problem:** TTS ist langsam (~1-3s pro Narration). Wenn der gleiche Text nochmal kommt, will man nicht wieder warten.

**LÃ¶sung:** SHA256-basierter LRU-Cache.

```python
# Cache-Key = SHA256(text + "|" + voice)
# Selber Text mit anderer Stimme = anderer Cache-Eintrag

def get(text, voice):
    key = sha256(text + "|" + voice)
    path = ~/.cache/multikanal/{key}.wav
    if path.exists():
        path.touch()  # LRU: Access-Time aktualisieren
        return path
    return None

def put(text, voice, wav_path):
    key = sha256(text + "|" + voice)
    shutil.copy2(wav_path, ~/.cache/multikanal/{key}.wav)
    evict_if_needed()  # Wenn > 500 EintrÃ¤ge: Ã¤lteste lÃ¶schen
```

**LRU-Eviction:**
- Max 500 .wav-Dateien im Cache
- Wenn voll: sortiere nach Access-Time, lÃ¶sche die Ã¤ltesten
- `path.touch()` bei jedem `get()` â†’ oft genutzte Dateien Ã¼berleben

**Praktischer Effekt:** "Tests bestanden" nach dem dritten Mal = instant Playback (0ms statt 2000ms).

### 6.4 Playback (`tts/playback.py`)

**Auch hier: Fallback-Chain.**

```
paplay (PulseAudio-native, beste Integration)
  â†’ ffplay (FFmpeg, universell)
    â†’ aplay (ALSA-direkt, Basis)
```

**Features:**
- **PulseAudio Sink Selection:** `PULSE_SINK` Environment-Variable â†’ Audio an bestimmten Ausgang
- **Volume Control:** 0.1 bis 2.0 (normalisiert auf paplay: 3277â€“65536, ffplay: 5â€“100)
- **Non-blocking:** `subprocess.Popen` + `wait()` â†’ blockiert nur den Queue-Worker, nicht den Daemon

### 6.5 Audio Queue (im Daemon)

```python
_audio_queue = asyncio.Queue(maxsize=5)

async def _audio_queue_worker():
    while True:
        wav_path, audio_cfg = await _audio_queue.get()
        await asyncio.to_thread(_player.play, wav_path, sink, volume)
        _audio_queue.task_done()
```

**Warum eine Queue?**
- Ohne Queue: 3 Narrations kommen gleichzeitig â†’ 3 Stimmen gleichzeitig â†’ Chaos
- Mit Queue: FIFO, eine nach der anderen, maximal 5 warten
- Queue voll? â†’ Narration wird Ã¼bersprungen (besser als endlos wachsende Queue)

### 6.6 Der komplette Audio-Pfad

```
"3 files changed, 47 insertions" (Claude Code Output)
    â†“
[Filter] â†’ "3 files changed, 47 insertions"
    â†“
[Cache Check] â†’ Miss
    â†“
[MiniMax LLM] + audio_prompt.md System-Prompt
    â†’ "Drei Dateien angepasst, Login sollte jetzt stabil laufen."
    â†“
[Eval Log] â†’ {"info_density": 0.78, "filler_count": 0, ...}
    â†“
[Prefix] â†’ "BepBup: Drei Dateien angepasst, Login sollte jetzt stabil laufen."
    â†“
[Edge TTS] â†’ Florian-Stimme, rate=+0%, pitch=+0Hz â†’ /tmp/multikanal_abc.wav
    â†“
[Cache Put] â†’ ~/.cache/multikanal/sha256hash.wav
    â†“
[Audio Queue] â†’ Warte bis vorheriges Audio fertig
    â†“
[paplay] â†’ Lautsprecher ğŸ”Š
```

---

## 7. Technologie-Stack

| Komponente | Tech | Warum diese? |
|-----------|------|-------------|
| HTTP-Server | **FastAPI** + **Uvicorn** | Async-first, auto-Validierung via Pydantic, schnell |
| HTTP-Client | **HTTPX** | Async + Sync, moderne API, gutes Error Handling |
| Config | **PyYAML** + **.env** | YAML fÃ¼r Struktur, .env fÃ¼r Secrets (getrennt!) |
| File-Watch | **Watchdog** | OS-native Events (inotify auf Linux), robust |
| TTS primÃ¤r | **Edge TTS** | Kostenlos, Neuralvoices, 10+ deutsche Stimmen |
| TTS fallback | **Piper** | Offline, ONNX-basiert, Open Source |
| LLM primÃ¤r | **MiniMax M2.1** | Billig (~$0.15/1M tokens), schnell (~3s) |
| LLM fallback | **Ollama** | Gratis, lokal, kein Internet nÃ¶tig |
| Audio | **PulseAudio** (paplay) | Linux-Standard, Sink-Routing, Volume-Control |
| Prozess-Mgmt | **systemd** | User-Service, Autostart, Journal-Logging |

---

## 8. Kosten

| Was | Kosten | Wann |
|-----|--------|------|
| MiniMax Narration | ~$0.008 / Aufruf | Pro Tool-Ergebnis |
| Edge TTS | $0 | Immer |
| Ollama | $0 (Strom) | Nur Fallback |
| Piper | $0 | Nur Fallback |
| **Gesamt (~100 Narrations/Tag)** | **~$0.50/Monat** | |

---

## 9. StÃ¤rken & SchwÃ¤chen

### StÃ¤rken
- **Fail-Soft auf jeder Ebene** â€” 4 LLM-Fallbacks, 3 TTS-Fallbacks, 3 Playback-Fallbacks
- **Nie blockiert** â€” Hooks exit sofort, Daemon ist async, Queue verhindert Overlap
- **Messbar** â€” Eval-Logging macht Prompt-Tuning datengetrieben statt BauchgefÃ¼hl
- **Hot-Reload** â€” Prompt Ã¤ndern = sofort aktiv, kein Neustart
- **Multi-Agent-Aware** â€” Jeder Agent eigene Stimme, eigene Speed/Pitch-Settings
- **GÃ¼nstig** â€” Unter $1/Monat bei normalem Gebrauch

### SchwÃ¤chen
- **MiniMax-AbhÃ¤ngigkeit** â€” Wenn API-Key ungÃ¼ltig UND Ollama nicht lÃ¤uft â†’ nur Templates
- **Prompt-Tuning = Kunst** â€” Funktioniert am besten auf Deutsch
- **OpenCode SSE** â€” Keine automatische Reconnection bei Server-Neustart (manueller Workaround: Polling)
- **Keine Queue-Persistenz** â€” Daemon-Crash = wartende Narrations verloren
- **Edge TTS braucht Internet** â€” Offline nur mit Piper (schlechtere QualitÃ¤t)
